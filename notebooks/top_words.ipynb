{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/babi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/babi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SlackDataLoader(\"../data\")\n",
    "all_channels_message = {}\n",
    "for channel in data_loader.channels:\n",
    "    channel_messages = utils.get_messages_on_channel(f\"../data/{channel[\"name\"]}\")\n",
    "    all_channels_message[channel[\"name\"]] = channel_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Extract and remove URLs\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "    \n",
    "    text = re.sub(r'<@.*?>', '', text)\n",
    "\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_channel_data(channel_data):\n",
    "    preprocessed_data = []\n",
    "    for msg in channel_data:\n",
    "        pre_procssed = preprocess_text(msg['text'])\n",
    "        if pre_procssed:\n",
    "            preprocessed_data.append({'text': pre_procssed, 'ts': msg['ts']})\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "# Apply preprocessing to each channel's data\n",
    "preprocessed_channel_data = {}\n",
    "\n",
    "for channel, data in all_channels_message.items():\n",
    "    preprocessed_channel_data[channel] = preprocess_channel_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for all-community-building: [array([['joy', 'man', 'like', 'hi', 'one', 'rollingonthefloorlaughing',\n",
      "        'morning', 'good', 'yes', 'hello']], dtype=object)]\n",
      "Top words for all-technical-support: [array([['check', 'okay', 'please', 'link', 'thanks', 'thank', 'try',\n",
      "        'think', 'joined', 'channel']], dtype=object)]\n",
      "Top words for all-career-exercises: [array([['link', 'one', 'please', 'morning', 'thank', 'yes', 'joined',\n",
      "        'good', 'thanks', 'channel']], dtype=object)]\n",
      "Top words for all-resources: [array([['data', 'good', 'helpful', 'found', 'resource', 'help', 'thank',\n",
      "        'thanks', 'joined', 'channel']], dtype=object)]\n",
      "Top words for random: [array([['man', 'joy', 'grin', 'one', 'think', 'time', 'yes',\n",
      "        'rollingonthefloorlaughing', 'joined', 'channel']], dtype=object)]\n",
      "Top words for all-ideas: [array([['mean', 'hello', 'glad', 'market', 'dont', 'good', 'one',\n",
      "        'think', 'joined', 'channel']], dtype=object)]\n",
      "Top words for all-week1: [array([['task', 'link', 'think', 'help', 'use', 'data', 'joined',\n",
      "        'channel', 'thanks', 'thank']], dtype=object)]\n",
      "Top words for all-broadcast: [array([['join', 'standup', 'please', 'independent', 'work', 'pm', 'next',\n",
      "        'time', 'session', 'challenge']], dtype=object)]\n",
      "Top words for tenx-bot: [array([['added', 'appdmb6', 'integration', 'channel', 'joined']],\n",
      "      dtype=object)]\n",
      "Top words for team-10: [array([['right', 'im', 'problem', 'task', 'think', 'okay', 'meeting',\n",
      "        'thanks', 'yes', 'alright']], dtype=object)]\n",
      "Top words for all-week2: [array([['version', 'try', 'error', 'data', 'yes', 'thank', 'thanks',\n",
      "        'dvc', 'channel', 'joined']], dtype=object)]\n",
      "Top words for week2-group: [array([['archived', 'channel', 'joined']], dtype=object)]\n",
      "Top words for ab_test-group: [array([['time', 'merge', 'great', 'working', 'yes', 'task', 'repo',\n",
      "        'work', 'branch', 'okay']], dtype=object)]\n",
      "Top words for week-2-group-8: [array([['invitation', 'eko', 'yes', 'work', 'great', 'please', 'channel',\n",
      "        'joined', 'guys', 'ok']], dtype=object)]\n",
      "Top words for dsa-sql: [array([['started', 'please', 'use', 'sql', 'resources', 'thanks',\n",
      "        'contest', 'challenge', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-week3: [array([['file', 'try', 'think', 'dvc', 'thank', 'thanks', 'use', 'data',\n",
      "        'channel', 'joined']], dtype=object)]\n",
      "Top words for week4-teamwork: [array([['time', 'bro', 'think', 'task', 'sure', 'yes', 'guys', 'okay',\n",
      "        'joined', 'channel']], dtype=object)]\n",
      "Top words for study-group: [array([['sounds', 'good', 'im', 'okay', 'well', 'hi', 'guys', 'hello',\n",
      "        'joined', 'channel']], dtype=object)]\n",
      "Top words for happy-new-year-study-group: [array([['link', 'think', 'possible', 'data', 'hello', 'meeting', 'sure',\n",
      "        'good', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-week4: [array([['great', 'thank', 'yeah', 'api', 'use', 'yes', 'data', 'thanks',\n",
      "        'channel', 'joined']], dtype=object)]\n",
      "Top words for batch6_week4_studygroup: [array([['fill', 'free', 'invite', 'others', 'digital', 'hey',\n",
      "        'librarywelcome', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-week5: [array([['help', 'data', 'dbt', 'thank', 'docker', 'airflow', 'yes',\n",
      "        'thanks', 'joined', 'channel']], dtype=object)]\n",
      "Top words for all-week6: [array([['thank', 'testnet', 'using', 'good', 'error', 'try', 'thanks',\n",
      "        'sandbox', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-week7: [array([['getting', 'file', 'ssh', 'key', 'permission', 'thanks', 'yes',\n",
      "        'error', 'channel', 'joined']], dtype=object)]\n",
      "Top words for kafka_de: [array([['using', 'think', 'error', 'ok', 'one', 'meeting', 'sure',\n",
      "        'working', 'try', 'yes']], dtype=object)]\n",
      "Top words for all-week8: [array([['session', 'get', 'yes', 'find', 'pray', 'morning', 'good',\n",
      "        'thank', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-week9: [array([['one', 'order', 'submission', 'data', 'yes', 'trip', 'thank',\n",
      "        'thanks', 'joined', 'channel']], dtype=object)]\n",
      "Top words for all-week10: [array([['try', 'install', 'network', 'use', 'let', 'yes', 'thank',\n",
      "        'thanks', 'channel', 'joined']], dtype=object)]\n",
      "Top words for week-11-group4: [array([['thank', 'thanks', 'yes', 'instance', 'ok', 'guys', 'think',\n",
      "        'joined', 'channel', 'try']], dtype=object)]\n",
      "Top words for gokada-challenge-presentation: [array([['map', 'link', 'solid', 'order', 'present', 'presentation',\n",
      "        'time', 'today', 'joined', 'channel']], dtype=object)]\n",
      "Top words for all-week11: [array([['working', 'im', 'yes', 'please', 'file', 'error', 'instance',\n",
      "        'thank', 'channel', 'joined']], dtype=object)]\n",
      "Top words for adludios-challange: [array([['thanks', 'nice', 'okay', 'meeting', 'try', 'guys', 'good',\n",
      "        'think', 'working', 'yes']], dtype=object)]\n",
      "Top words for chang-w11: [array([['instance', 'try', 'working', 'yes', 'everyone', 'morning',\n",
      "        'meeting', 'sure', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-ml-week12: [array([['drive', 'dvc', 'get', 'notebook', 'use', 'raisedhands',\n",
      "        'struggling',\n",
      "        'textdowhy20breaks20down20causal20inferenceidentify2c20estimate2c20and20refute',\n",
      "        'channel', 'joined']], dtype=object)]\n",
      "Top words for all-de-week12: [array([['channel', 'joined']], dtype=object)]\n",
      "Top words for all-week12: [array([['install', 'data', 'try', 'standup', 'today', 'okay', 'thank',\n",
      "        'error', 'channel', 'joined']], dtype=object)]\n",
      "Top words for all-web3-week12: [array([['get', 'given', 'print', 'see', 'trainees', 'asset', 'status',\n",
      "        'optedin', 'channel', 'joined']], dtype=object)]\n",
      "Top words for machine-learning: [array([['channel', 'joined']], dtype=object)]\n",
      "Top words for data-engineering: [array([['channel', 'joined']], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of top words to extract for each channel\n",
    "num_top_words = 10\n",
    "\n",
    "# Define a function to get the top words from TF-IDF features\n",
    "def get_top_words(channel_data):\n",
    "    # Extract text from each message\n",
    "    texts = [msg['text'] for msg in channel_data]\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Vectorization using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Get feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get the top words based on TF-IDF weights\n",
    "    top_words = [feature_names[i] for i in X.sum(axis=0).argsort(axis=1)[0, -num_top_words:][::-1]] # type: ignore\n",
    "\n",
    "    return top_words\n",
    "\n",
    "# Get top words for each channel\n",
    "top_words_by_channel = {channel: get_top_words(data) for channel, data in preprocessed_channel_data.items()}\n",
    "\n",
    "# Print the top words for each channel\n",
    "for channel, words in top_words_by_channel.items():\n",
    "    print(f\"Top words for {channel}: {words}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenx_week0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
