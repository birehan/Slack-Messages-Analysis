{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import  sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import modules from src\n",
    "rpath = os.path.abspath('..')\n",
    "if rpath not in sys.path:\n",
    "    sys.path.insert(0, rpath)\n",
    "\n",
    "from src.loader import SlackDataLoader\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SlackDataLoader(\"../data\")\n",
    "\n",
    "def get_channel_messages(channel):\n",
    "    channel_messages = utils.get_messages_on_channel(f\"../data/{channel}\") \n",
    "    print(channel_messages)  \n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame(channel_messages)\n",
    "    return channel_messages\n",
    "\n",
    "def get_all_channels_message():\n",
    "    dfs = []  # List to store individual DataFrames\n",
    "\n",
    "    for channel in data_loader.channels:\n",
    "        dfs.append(get_channel_messages(channel))\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    result_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# all_channels_message = {}\n",
    "# for channel in data_loader.channels:\n",
    "#     channel_messages = utils.get_messages_on_channel(f\"../data/{channel[\"name\"]}\")\n",
    "#     all_channels_message[channel[\"name\"]] = channel_messages\n",
    "\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # Iterate through the channels and messages\n",
    "# for channel, messages in all_channels_message.items():\n",
    "#     for message in messages:\n",
    "#         data.append({'channel': channel, 'ts': message['ts'], 'text': message['text']})\n",
    "\n",
    "# # Create a Pandas DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "# print(df.columns)\n",
    "# print(df.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Extract and remove URLs\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "\n",
    "    text = re.sub(r'<@.*?>', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(df):\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    sentence_list = [tweet for tweet in df['cleaned_text']]\n",
    "    word_list = [sent.split() for sent in sentence_list]\n",
    "\n",
    "    #Create dictionary which contains Id and word\n",
    "    word_to_id = corpora.Dictionary(word_list) #generate unique tokens\n",
    "    corpus = [word_to_id.doc2bow(tweet) for tweet in word_list]\n",
    "    \n",
    "    return df, word_list, word_to_id, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(corpus, word_to_id):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                        id2word=word_to_id,\n",
    "                                        num_topics=5,\n",
    "                                        random_state=100,\n",
    "                                        update_every=1,\n",
    "                                        chunksize=100,\n",
    "                                        passes=10,\n",
    "                                        alpha='auto',\n",
    "                                        per_word_topics=True)    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_topics(lda_model):\n",
    "    pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(lda_model, corpus, word_list, word_to_id):\n",
    "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "    doc_lda = lda_model[corpus]\n",
    "\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=word_to_id, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print('\\n Lda model Coherence Score/Accuracy on Tweets: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def get_top_topics(df):\n",
    "    df,word_list, word_to_id, corpus = prepare_data(df)\n",
    "    # lda_model = build_model(corpus, word_to_id)\n",
    "\n",
    "    # show_topics(lda_model)\n",
    "\n",
    "    # print(\"Perform model analysis\")\n",
    "    # model_analysis(lda_model,corpus, word_list, word_to_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 topics of the different channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': \"<!here> <@U03V8LHPDME> <@U03UKL27B0R> I appreciate if anyone can give me an idea on label encoding 'Bearer Id, IMEI, IMSI and MSISDN/Number columns'? May it have a different meaning if I encode them like Handset Manufacturer and Handset Type columns?\", 'ts': '1661499578.568379'}, {'text': '<http://meet.google.com/ysb-kjdn-hpp>', 'ts': '1661500664.614519'}, {'text': 'so how could I extract a dataset from the groupby function so I could preform normalization in the data ???', 'ts': '1661503227.480699'}, {'text': 'the groupby function returns you a df and you can assign it to a variable to do what you want to do.', 'ts': '1661503335.973589'}, {'text': 'join us here <http://meet.google.com/ysb-kjdn-hpp> for the tenx demo', 'ts': '1661504654.183159'}, {'text': 'The tenx session has started <http://meet.google.com/ysb-kjdn-hpp>', 'ts': '1661504880.646299'}, {'text': 'are we supposed to store the features on a database (MySQL)? if yes all of the features or specific columns.', 'ts': '1661505253.078859'}, {'text': 'so how I could do normalization in this df??', 'ts': '1661505284.020299'}, {'text': '', 'ts': '1661505290.581149'}, {'text': '<!channel> Guidelines from tenx team on github submissions (Please review them): <https://10academy.notion.site/Github-Submissions-d113da4646be47cc924aa669a796b9ee>', 'ts': '1661506591.829159'}, {'text': '<!here> Weekly contest has started!', 'ts': '1661506666.913679'}, {'text': 'I have change the structure of my repo , what I need to do ti ensure that my submit is been graded', 'ts': '1661506707.856939'}, {'text': 'Same link <http://meet.google.com/ysb-kjdn-hpp>', 'ts': '1661506743.117269'}, {'text': '<@U03UKL27B0R> May I submit you the code or simply submit to Hackerank?', 'ts': '1661508203.657499'}, {'text': 'Please submit directly from the platform.', 'ts': '1661508227.196019'}, {'text': 'You mean yo can see it from the platform?', 'ts': '1661508249.611199'}, {'text': 'Yes.', 'ts': '1661508268.753829'}, {'text': 'I tried to do for all features but there are features I dropped for modeling.', 'ts': '1661509538.156139'}, {'text': 'yeah . I understand we have to aggregate features depending on the required task.\\nBut finally we are expected to develop a dashboard right? That is where I got the question.', 'ts': '1661509622.326779'}, {'text': 'the dashboard needs specific important features to be fetched from db. I think the db should contain whole data. May it make sense?', 'ts': '1661509714.040009'}, {'text': 'are you done on feature selection with fit()?', 'ts': '1661509757.406249'}, {'text': 'okay  ,the columns are a lot but if we have to store all of them then be it.\\nI do not  understand what second question is refereeing to. You can DM me.', 'ts': '1661510008.365039'}, {'text': 'pls help', 'ts': '1661516090.238719'}, {'text': 'may you reshare us the screenshot?', 'ts': '1661516813.361089'}, {'text': \"I have a basic question. Are we supposed to have 2 GitHub repos for tomorrow's submission?\", 'ts': '1661523387.610829'}, {'text': 'no <@U03UJKJGRAQ> just a different link on the repo', 'ts': '1661523452.766419'}, {'text': \"<@U03UFV7HFNF> I don't quite understand. 2 Links to the same repo. Can you explain what are these 2 links?\", 'ts': '1661523607.252799'}, {'text': 'Actually you are submitting the same repo link you submitted on Wednesday.', 'ts': '1661524286.376329'}, {'text': 'can someone *<!here>* explain to me what the \"Data analysis and coding\" submission is about?', 'ts': '1661524743.201779'}, {'text': 'All Jupyter files and related links', 'ts': '1661524816.892179'}, {'text': 'isnt that already in the repository?', 'ts': '1661524863.225659'}, {'text': '<@U03U9DB7REG> Yes.\\nBut the submission links on google classroom is asking for 2 GitHub links:\\n1. A GitHub link to your Data analysis code.\\n2. Submit a GitHub link to your Dashboard code\\nAre these links to the same repo?', 'ts': '1661524952.151909'}, {'text': \"Yeah you are  right .I was asking yesterday but I think they want it to be easy to find and more specific. Anyway let's tag <@U03TEPYRM2P> here.\", 'ts': '1661525041.386479'}, {'text': 'thanks man', 'ts': '1661525098.791179'}, {'text': 'You are welcome', 'ts': '1661525111.585109'}, {'text': 'So your project should have different directories or branches for these tasks. If you have a directory or a branch for you dashboard then you submit the link to that directory and the same for the data analysis.', 'ts': '1661525125.706119'}, {'text': \"You will only be encoding features that are going to be fed to your model. The above features are just a unique identifiers which won't be useful for that model that you are going to build.\", 'ts': '1661526101.609229'}, {'text': 'What is the issue that you are facing <@U03UJH1EQQL> ?', 'ts': '1661527623.393139'}, {'text': 'how do i use a column to sort a dataframe?', 'ts': '1661533995.118179'}, {'text': \"something like this can help you.\\n```df.sort_values(by='column name)', ascending=False)```\", 'ts': '1661534046.218649'}, {'text': \"`df.sort_values('column_name', ascending=False)`\", 'ts': '1661534106.925139'}, {'text': 'got it thank you', 'ts': '1661534131.480319'}, {'text': \"So as far as we're concerned, we submit same links for the dashboard and the data analysis coding?\", 'ts': '1661535631.430329'}, {'text': \"Is normalizing supposed to remove outliers? I've tried 3 normalizing methods now and i still have outliers\", 'ts': '1661538089.273299'}, {'text': 'Do I have to remove outliers before performing k-clustering?', 'ts': '1661538247.155559'}, {'text': '<https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029>', 'ts': '1661538356.008169'}, {'text': 'If you do, You will get a better clustering of your data', 'ts': '1661538393.661259'}, {'text': 'How do I go from having just jupyter notebooks to having a dashboard?', 'ts': '1661542522.572009'}, {'text': 'You can load your model in another python file and work on dashboard from there.', 'ts': '1661542725.883969'}, {'text': 'Using streamlit, you can showcase your findings there with more interactivity.', 'ts': '1661543001.355069'}, {'text': 'Can you list out the steps please', 'ts': '1661546048.762779'}, {'text': 'Are we supposed to use avg TCP value or total TCP value for task 3.2?', 'ts': '1661546948.594359'}, {'text': 'Average i think', 'ts': '1661555750.419889'}, {'text': \"How do I go from having just jupyter notebooks to having a dashboard? What's is the very first step to take?\", 'ts': '1661580554.514329'}, {'text': 'You can start making visualizations, or loading your models and make predictions', 'ts': '1661582414.282899'}, {'text': \"OK here goes\\n\\n1. Is to actually know or plan what you want to visualize or show on your dashboard. Usually, almost all the things I did for myself.\\n2. Next, you install stream lit or your desired mode of visualizer modules like Heroku. I prefer stream lit because I never really worked with Heroku.\\n3. After installing everything, you then use all your pre-written scripts here for your advantage. Meaning you can directly use all the methods and helper functions you have already written for your dashboard demonstration. Especially all your data visualization scripts.\\n4. Then you will create a python file or files containing the streamlit module to do the dashboard. At this point, it all comes to using the streamlit module to make and show your dashboard. You don't have to exactly show each and everything as you did in your notebook. You can make many other kinds of visualization in streamlit.\\n5. Finally, after you completed what you want to show using your python file or files, you then can just use the official streamlit website to host your demonstrations publicly.\\nHere are some reference links\\n1. Getting started with streamlit. <https://docs.streamlit.io/library/get-started>\\n2. Hosting apps to streamlit. <https://docs.streamlit.io/streamlit-cloud/get-started/deploy-an-app>\\nI hope this helps. Try and look over the two resources. They helped me alot.\", 'ts': '1661582949.956319'}, {'text': '<https://10academybatch6.slack.com/archives/C03TEQQS9NF/p1661582949956319?thread_ts=1661542522.572009&amp;cid=C03TEQQS9NF>', 'ts': '1661582994.366669'}, {'text': \"<@U03UVHCV6KB>!!!!!!!! Thank you so much! I'll surely go over them.\", 'ts': '1661583435.117389'}, {'text': 'This is what ive been looking for, thank you guys :raised_hands::skin-tone-4:', 'ts': '1661583566.869679'}, {'text': 'I guess it all depends on two things. The distribution of the data points and the model you are feeding your data to', 'ts': '1661411737.329789'}, {'text': 'Good morning everyone', 'ts': '1661413141.333999'}, {'text': 'Good Morning', 'ts': '1661413191.727669'}, {'text': 'Does it make sense to fill missing value for a columns where there is more than 50% missing ?\\nWhat if the columns is a variable of interest?', 'ts': '1661413207.776949'}, {'text': 'No it will makes your model produce undesirable outputs. I dropped columns having more than 30%.', 'ts': '1661413418.882979'}, {'text': 'Mee too.. Alright', 'ts': '1661413491.911979'}, {'text': 'StandUp has started', 'ts': '1661414460.632249'}, {'text': '<https://meet.google.com/ysb-kjdn-hpp>', 'ts': '1661414463.340649'}, {'text': 'Any one who makes me clarification on Careers Exercise?', 'ts': '1661418826.065749'}, {'text': 'I like the idea of the contest very much. I hope that will help us for our problem solving skill. Thank you <@U03UKL27B0R> for the presentation.', 'ts': '1661418850.315159'}, {'text': '<http://docs.google.com/presentation/d/1Dao0753Jvk73wBwCqBrWldu9APaqX-eREY09B2wtk3Y|http://docs.google.com/presentation/d/1Dao0753Jvk73wBwCqBrWldu9APaqX-eREY09B2wtk3Y>', 'ts': '1661418871.524719'}, {'text': 'Please ask your question(s) on <#C03T89KDGA2|all-career-exercises>', 'ts': '1661419171.060219'}, {'text': '<https://10academybatch6.slack.com/|https://10academybatch6.slack.com/>', 'ts': '1661423036.988669'}, {'text': '<https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670|https://towardsdatascience.com/create-your-own-k-means-clustering-algorithm-in-python-d7d4c9077670>', 'ts': '1661423075.584399'}, {'text': 'kindly post resources in the all-resources channel', 'ts': '1661424505.301169'}, {'text': '<#C03T89PMJKG|all-resources>', 'ts': '1661424538.664509'}, {'text': '<@U03UJH1EQQL>', 'ts': '1661426047.375439'}, {'text': 'Hello everyone and dear team,\\nWe have plenty tasks that are not really easy to do and not small as well. Any advice about how to successfully manage in this kind of situation will be helpful.\\nThanks in advance.', 'ts': '1661426369.859829'}, {'text': 'I suggest that if  the managing team made a share experience session with one of 10 Academy alumni to share his experience on how he manage to deliver the tasks and time management also and what we should do to make the most out of this 3 months', 'ts': '1661426750.680949'}, {'text': 'I love this Idea ', 'ts': '1661426808.047799'}, {'text': 'I think this is going to happen in due time. They said there will be guest talk sessions.', 'ts': '1661426927.376189'}, {'text': '<@U03UG4Q7V42> Any Idea when will that be ', 'ts': '1661426983.515609'}, {'text': 'I have no idea on that <@U03V61VGQG0>', 'ts': '1661427102.704479'}, {'text': 'This will be very helpful.', 'ts': '1661427287.063439'}, {'text': \"I feel you. My daily routine has been in chaos cos of the workload, But I think a good system will help with time. Whenever we got the chance to have a session with the alumni, we'll ask them how they go about it.\", 'ts': '1661428297.900169'}, {'text': \"Starting today though, I'm trying to focus on one task at a time and see how that goes\", 'ts': '1661428359.965779'}, {'text': '<@U03UH397319> <@U03UJN29Y4C> Help us by writing a bit more about what challenges you face.', 'ts': '1661428487.462369'}, {'text': 'All of your tutors are 10 Ac alumni - ask them directly for help.', 'ts': '1661428505.646659'}, {'text': \"I think the best thing to do here is to have your own schedule. Everyone has a different attitude and approach toward things. And therefore the Aluminis advice might not be applicable to some and might be even bottom line impossible for others. And therefore I would like to recommend to you based on my imperceptible wisdom to have your own schedule at the beginning of the week. Take a look at that week's challenge doc. see what we are supposed to deliver (on the deliverables section) and after you understand what we need to complete and by when you then plan out the week. Now I think the most important part about the schedule is following it, you might not complete everything on time, and you might finish some before the allotted time frame ends, But for the unfinished tasks don't waste too much unscheduled time on them, as they will definitely take up the other tasks time. So by planning and following your schedule even if you don't complete everything you might manage to get lots done.\\n\\nLast but not least, don't forget other areas of your life in the meantime. Everyday balance is very necessary and is definitely, at least for me, what constitutes the good life.\\n\\nGood luck, and Godspeed!\", 'ts': '1661428819.021849'}, {'text': 'I believe that every person has it its own way to approach things, while some routs will not be appropriate for some other will be. And I totally believe that sharing our experience would benefit others like what you did, beside having a diversified experience examples could help you creat your own route. Thank you again for sharing your thoughts and I wish you all the best', 'ts': '1661429127.611689'}, {'text': 'same here', 'ts': '1661430478.029659'}, {'text': 'And just to make myself clear the previous message was just a thought and not in any way resistance to the message you were transmitting, on how Aluminis sharing their schedule', 'ts': '1661430633.306019'}, {'text': '<https://meet.google.com/ysb-kjdn-hpp> Tutorial session about to start', 'ts': '1661432555.537529'}, {'text': 'I appreciate you response', 'ts': '1661436050.217269'}, {'text': 'hey, I want the link of todays tutorial', 'ts': '1661440505.831539'}, {'text': 'it will be uploaded soon', 'ts': '1661440550.524179'}, {'text': 'I could find it in <#C03T89PMJKG|all-resources>?', 'ts': '1661440635.067999'}, {'text': '<https://10academybatch6.slack.com/archives/C03U4J8J4LQ/p1661446976891059>', 'ts': '1661450939.737479'}, {'text': 'What are the columns that can help me to calculate the average throughput? I was thinking of using the columns with (kbps) as they indicate transmission rate, but I am a bit confused about the others for example, &lt;Activity Duration DL (ms)&gt;.', 'ts': '1661451310.312569'}, {'text': 'taken from the data field description file.\\n\\nyou can get the data  here\\n\\n<https://docs.google.com/spreadsheets/d/1wY7YZwyZ_r_8xMUe_N2ZQled4RjP0_T6/edit#gid=497912695>', 'ts': '1661451460.409879'}, {'text': 'I think avg bearer TP ul and dl', 'ts': '1661451470.468449'}, {'text': 'I need some one who understood task two to ask more questions??', 'ts': '1661451551.074739'}, {'text': 'got it, thank you <@U03UVHCV6KB> and <@U03U1FNPEUX>', 'ts': '1661451593.050669'}, {'text': 'What about task two man?', 'ts': '1661451821.547979'}, {'text': 'I did not understand what is the deliverables,', 'ts': '1661451948.700039'}, {'text': 'how I am going to aggregate the data and how I am going to cluster it', 'ts': '1661452000.070599'}, {'text': 'I think aggregation is performed on the UL and DL datum. You simply aggregate or add them and put them in a new column. For example Gaming DL + gaming UL becomes total gaming data. And for the clustering you use k-means. K=3. Ie 3 clusters. You can refer the following for clustering.', 'ts': '1661452593.095239'}, {'text': '<https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/|https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/>', 'ts': '1661452602.171209'}, {'text': 'What is everybody using to measure session frequency in the data?', 'ts': '1661452626.763289'}, {'text': 'In the current dataset you’re expected to track the user’s engagement using the following engagement metrics:\\n• sessions frequency \\n• the duration of the session \\n• the sessions total traffic (download and upload (bytes))\\nTask 2.1 - Based on the above submit python script and slide :\\n• Aggregate the above metrics per customer id (MSISDN) and report the top 10 customers per engagement metric \\n• Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \\n• Compute the minimum, maximum, average &amp; total non- normalized metrics for each cluster. Interpret your results visually with accompanying text explaining your findings.\\n', 'ts': '1661452992.700209'}, {'text': 'could you elaborate more in those points ?', 'ts': '1661453010.555379'}, {'text': 'I just counted number of session per user.', 'ts': '1661453284.344849'}, {'text': 'how ?', 'ts': '1661453408.864239'}, {'text': \"when I agg the data by `groupby` function it doesn't give me the total num per customer id\", 'ts': '1661453465.888119'}, {'text': 'frequency = df.groupby(‘MSISDN/Number’).agg(\\n    {‘Bearer Id’: ‘count’}).reset_index() have you tried it like this', 'ts': '1661453506.370809'}, {'text': \"no  I will try it , but isn't agg() used only for mean and median ?\", 'ts': '1661453602.486419'}, {'text': \"I think it's how many times the bearer id appeared for the same msisdn number. That's the XDR session per msisdn(or the unique user)\", 'ts': '1661453612.405129'}, {'text': 'I considered the MSISDN/Number as a customer id. but it has duplication which indicates the frequency. for example if a user with id 3345 appeared twice in the dataset it mean that it has 2 freq.', 'ts': '1661453615.203069'}, {'text': '<@U03V6HMRPGQ> you can also use it to calculate sum, count and other functions..', 'ts': '1661453729.923299'}, {'text': 'do you have link for more depth ?', 'ts': '1661453968.628319'}, {'text': \"Duration is given in the table.\\nSessions total traffic can be calculated by adding total UL and total DL.\\nThen using the three metrics, you're supposed to find out top 10 users.\", 'ts': '1661454080.725839'}, {'text': 'I don’t know if this could help as it has helped me\\n<https://www.thecodeteacher.com/question/48234/python---Group-dataframe-and-get-sum-AND-count>?', 'ts': '1661454243.465679'}, {'text': 'The general idea is, to first group the data by the msisdn/number and then sum/count the values related to a single msisdn/number.', 'ts': '1661454405.654879'}, {'text': 'thank Degaga', 'ts': '1661454555.532449'}, {'text': 'is this enough ?', 'ts': '1661454675.879979'}, {'text': 'and if so how could I run K-mean clustering func?', 'ts': '1661454713.767299'}, {'text': 'Not enough. Do that for the remaining two metrics too, (session freq and traffic', 'ts': '1661455420.999509'}, {'text': '<https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/|https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/>\\nCheck this out', 'ts': '1661455462.582259'}, {'text': 'like this ?', 'ts': '1661456347.082039'}, {'text': '', 'ts': '1661456657.177469'}, {'text': 'Df_nor is normalized df', 'ts': '1661456679.860359'}, {'text': 'in normalization which method did you use?', 'ts': '1661457025.064989'}, {'text': 'Minmaxscaler() ', 'ts': '1661463622.794009'}, {'text': 'Hope you\\'re doing great guys.\\n\"Consider the engagement score as the Euclidean distance between the user data point &amp; the less engaged cluster\". According to this statement, when we say \\'and the less engaged cluster\\',  I hope that we are considering the mean value of the cluster.', 'ts': '1661470543.343519'}, {'text': 'Thank you <@U03U1FNPEUX> ', 'ts': '1661494027.744619'}, {'text': '<@U03U9DB7REG> has joined the channel', 'ts': '1660833152.776779'}, {'text': '<@U03UKL27B0R> has joined the channel', 'ts': '1660833720.941909'}, {'text': '<@U03UL5LSTG9> has joined the channel', 'ts': '1660838314.204679'}, {'text': '<@U03TT5KEYCF> has joined the channel', 'ts': '1660840086.510509'}, {'text': '<@U03UYNR4TS4> has joined the channel', 'ts': '1660854364.777769'}, {'text': '<@U03UP7V9Q57> has joined the channel', 'ts': '1660888172.562459'}, {'text': 'Thank you so much', 'ts': '1662015960.997039'}, {'text': '<@U03U1FNPEUX> has joined the channel', 'ts': '1661013391.256859'}, {'text': '<@U03U1FQKEMV> has joined the channel', 'ts': '1661013501.516649'}, {'text': '<@U03UG32J3PC> has joined the channel', 'ts': '1661013526.244959'}, {'text': '<@U03UFV7HFNF> has joined the channel', 'ts': '1661013549.135449'}, {'text': '<@U03U9EJR362> has joined the channel', 'ts': '1661013557.132919'}, {'text': '<@U03UFV7TUTV> has joined the channel', 'ts': '1661013570.047229'}, {'text': '<@U03TEPYRM2P> has joined the channel', 'ts': '1660301480.524989'}, {'text': '<@U03T89ACUUW> has joined the channel', 'ts': '1660301593.394369'}, {'text': 'I need help', 'ts': '1661324564.397769'}, {'text': 'I use vscode extension for notebook and it works great. Maybe you should try that?', 'ts': '1661324676.252859'}, {'text': \"you have to fetch and merge first (if you are sure you won't find merge conflict you can use `git pull` command) but if you are unsure please use `git fetch` then `git merge`\", 'ts': '1661324702.743019'}, {'text': 'Try \"git pull\" first', 'ts': '1661324717.409309'}, {'text': 'how do I combine handset type and handset manufacturer to identify the top 5 handsets per top 3 handset manufacturer?', 'ts': '1661325042.337519'}, {'text': 'You can select first a subset of the top 3 manufacturer and then find the top handsets', 'ts': '1661325357.761809'}, {'text': 'did u try to pull the remote repo first ?', 'ts': '1661325430.762469'}, {'text': 'thank you <@U03UH397319>', 'ts': '1661325546.868849'}, {'text': 'tnx i fixed it', 'ts': '1661325717.923899'}, {'text': 'tnx i fixed it', 'ts': '1661325750.718809'}, {'text': 'Hello, how do I perform correlation analysis and dimensionality reduction?', 'ts': '1661325901.633519'}, {'text': 'we are asked to calculate correlation  for the social media, google, email, youtube, netflix, gaming, and other.\\nyou can calculate corr..\\ncorr = df[&lt;selected column&gt;].corr()', 'ts': '1661326071.239929'}, {'text': 'use seaborn to draw it heat map', 'ts': '1661326084.422159'}, {'text': 'sns.heatmap(corr,\\ncmap= ‘Blues’,         xticklabels=corr.columns,          yticklabels=corr.columns)', 'ts': '1661326104.411699'}, {'text': 'This is for pca\\n<https://sebastianraschka.com/Articles/2014_pca_step_by_step.html>', 'ts': '1661327351.447879'}, {'text': 'Thank you <@U03UJGRN5E0> and <@U03UH397319>.', 'ts': '1661327724.902259'}, {'text': 'Daily standup is ongoing!', 'ts': '1661328114.291479'}, {'text': 'This also my be useful for pca:\\n<https://www.jcchouinard.com/pca-with-python/>', 'ts': '1661328768.703189'}, {'text': 'Thank You<@U03U9EJR362>  and <@U03UJN29Y4C>', 'ts': '1661328806.843719'}, {'text': 'PCA works only for numerical values. So, make sure to select the numeric columns before calculating the PCA\\nnumeric_df = df.select_dtypes(include=‘float64’)', 'ts': '1661329899.927129'}, {'text': 'did you solve this one? If not I can help you', 'ts': '1661330120.226419'}, {'text': 'Are we applying this on the whole dataset?  I thought we will only work withe those specific columns in task 1.1.', 'ts': '1661330136.820289'}, {'text': 'I asking that because those features are numeric', 'ts': '1661330176.781679'}, {'text': '<https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/#:~:text=for%20Dimensionality%20Reduction-,Dimensionality%20Reduction%20and%20PCA,to%20predict%20the%20target%20variable>.', 'ts': '1661330207.380779'}, {'text': '<https://dev.to/mage_ai/10-steps-to-build-and-optimize-a-ml-model-4a3h|https://dev.to/mage_ai/10-steps-to-build-and-optimize-a-ml-model-4a3h>', 'ts': '1661335557.543949'}, {'text': 'Anyone who can help me on dimensionality reduction?', 'ts': '1661337617.964079'}, {'text': '<https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad>', 'ts': '1661339230.665699'}, {'text': 'hello every one,  can anyone tell me how to do boxplot on categorical data?        when i tried it with object type, it raise such like error.', 'ts': '1661339607.372689'}, {'text': 'May be try grouping the data and then try to get a peek on that data.', 'ts': '1661339703.966749'}, {'text': 'I think it is impossible to have a boxplot for categorical data', 'ts': '1661339736.692049'}, {'text': 'Thank you all for answering, my problem is when I run the script it says that it’s not able to standard strings or something like that, give me some time to copy the error', 'ts': '1661339861.457819'}, {'text': 'Is it a must to have the cleaner functions in another file?', 'ts': '1661342356.435939'}, {'text': 'Would be better. Not to repeat your self', 'ts': '1661342403.577489'}, {'text': 'I think it is a best practice to do that. It makes the codes reusable', 'ts': '1661342405.738669'}, {'text': 'is this pair plot working for you guys? it just take eternity to finish, i waited but no plot yet\\n&lt;sns.pairplot(dfPair, hue=“IMEI”, height=2.5)&gt;??', 'ts': '1661343422.119079'}, {'text': \"IMEI is a numerical variable. Not a categorical one. It can't be used in 'hue'.\", 'ts': '1661343682.807199'}, {'text': 'What should I use? any suggestion ?', 'ts': '1661343803.311069'}, {'text': \"Using numerical variables for hue isn't best because sns will assume it's a continuous value. Instead, based on the type of analysis you are going to make choose a categorical feature with less number of categories to be have a better plot with clear categories.\", 'ts': '1661344830.223549'}, {'text': 'do I need to normalize and standardize all the columns ??', 'ts': '1661345033.715409'}, {'text': 'thank you <@U03UH397319> <@U03UKL27B0R> it is clear now.', 'ts': '1661345449.787689'}, {'text': 'Tutorial session on modeling has started.', 'ts': '1661346098.145759'}, {'text': 'Numerical features are the ones that are normally standardized or normalized. And you would perform feature scaling on the features that you are going to feed to your  model based on the feature importance /feature engineering.', 'ts': '1661349415.908529'}, {'text': '<https://www.youtube.com/watch?v=NRnaMCNOK7Y&amp;t=31s>', 'ts': '1661351109.673999'}, {'text': 'I need help :\\nHow do i  merge  a single file from branch into main in git?', 'ts': '1661352634.416549'}, {'text': 'Add and commit only that file and merge to main.', 'ts': '1661352822.333199'}, {'text': 'Okay thank you <@U03U9DB7REG>', 'ts': '1661352990.413059'}, {'text': 'or you can use the --patch command', 'ts': '1661353038.838499'}, {'text': 'I fixed the problem using interface. :pray: but i will try the --patch if the problem will happen again.', 'ts': '1661353668.734229'}, {'text': 'it is clear now', 'ts': '1661356372.991289'}, {'text': 'guys I could not find yesterday presentation ???', 'ts': '1661356409.972899'}, {'text': '**colab link', 'ts': '1661356446.928599'}, {'text': 'which was about EDA', 'ts': '1661356467.630929'}, {'text': '<https://youtu.be/J_6ZwHCT558>', 'ts': '1661356581.456039'}, {'text': 'am asking about the google colab link', 'ts': '1661356634.618049'}, {'text': '<https://colab.research.google.com/drive/1es71KWhYu4ty8m2ZKi4pub796JNyRh_g>', 'ts': '1661357154.570119'}, {'text': 'here you go good man', 'ts': '1661357159.810709'}, {'text': 'I am really thankful', 'ts': '1661359159.329929'}, {'text': 'when I grouped some of the data to achieve task 1.1, I faced some problems to plot the results. How I could achieve task 1.1 ???', 'ts': '1661359256.136679'}, {'text': 'how we are going to present this info the fulfill the task ???', 'ts': '1661360178.823919'}, {'text': '<@U03UJGP0C68>', 'ts': '1661360227.139959'}, {'text': 'What should I do? I tried updating seaborn', 'ts': '1661361162.900659'}, {'text': 'Can you elaborate on what you were trying to do?', 'ts': '1661361210.828679'}, {'text': 'is it enough for task 1 ???', 'ts': '1661361238.236009'}, {'text': \"I don't understand your question \", 'ts': '1661361620.604519'}, {'text': 'use\\n`sns.histplot()` \\ninstead', 'ts': '1661362063.379759'}, {'text': 'in which form I can submit my answers to this task ?', 'ts': '1661362362.388169'}, {'text': '<https://10academybatch6.slack.com/files/U03V6HMRPGQ/F03V06PEGHY/image.png>', 'ts': '1661362384.469589'}, {'text': 'like this ?', 'ts': '1661362393.943509'}, {'text': 'Thanks it worked', 'ts': '1661362995.368519'}, {'text': \"Just to be clear, today's submission would be in ppt or pdf?\", 'ts': '1661363814.937379'}, {'text': 'please submit pdf', 'ts': '1661363852.076589'}, {'text': 'when I plot my code it gives me like this', 'ts': '1661363877.823639'}, {'text': 'Okay, thank you', 'ts': '1661363879.528509'}, {'text': 'does my code fulfill task 1.1???', 'ts': '1661363947.024109'}, {'text': 'should I provide it like this ?', 'ts': '1661363964.059049'}, {'text': '<@U03TEPYRM2P> It says maximum of 15 slides, so are we supposed to convert the slide to pdf?', 'ts': '1661364021.178049'}, {'text': 'if it runs with out no issues it looks okay to me', 'ts': '1661364151.738289'}, {'text': 'I second <@U03UUR571A5>, i just submitted a ppt file', 'ts': '1661364182.591159'}, {'text': '<@U03U1FQKEMV> I am also working on a ppt file.', 'ts': '1661364214.066309'}, {'text': \"But in the plot function add the plot kind as argument like `plot(kind='bar')`\", 'ts': '1661364266.536179'}, {'text': 'ok, i will try to do it', 'ts': '1661364517.970399'}, {'text': 'If you have submitted ppt, no problem. However, you can convert ppt file to pdf', 'ts': '1661364792.142789'}, {'text': '• Conduct a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable and provide useful interpretation. \\nwhat this mean ?', 'ts': '1661365689.564309'}, {'text': \"it took alot of time and didn't present any outputs\", 'ts': '1661365731.532919'}, {'text': 'I think, it mean you have do statistical analysis such as mean, range, std, quartiles, percentiles, etc for each column separately', 'ts': '1661365951.051909'}, {'text': '<https://www.geeksforgeeks.org/exploratory-data-analysis-eda-types-and-tools/#:~:text=Univariate%20Non%2Dgraphical%3A%20this%20is,make%20observations%20about%20the%20population>.', 'ts': '1661366060.437909'}, {'text': 'so calculating the skew will do the job ??', 'ts': '1661366480.583889'}, {'text': 'Calculating the skew is one method of Non-Graphical Univariate Analysis. You might also find other methods and make your analysis rich', 'ts': '1661366620.645259'}, {'text': 'For univariate analysis,we have to select only relevant columns right?', 'ts': '1661367639.636029'}, {'text': 'Yes, I believe so.', 'ts': '1661367852.463199'}, {'text': 'Thanks, is it based on the output for task 1.1?', 'ts': '1661367998.773529'}, {'text': 'Yes', 'ts': '1661368225.092529'}, {'text': 'yes', 'ts': '1661369757.241069'}, {'text': 'I really got burnt out :broken_heart:, I hope that the tutors explain more about which tasks we should do and what not to do, I was working in the normalisation and standardisation for two day and not knowing that it will be explained today, and I wasted a lot of time figuring things that was explained easily today in the tutorial. I could use that time to do better in submitting todays task but unfortunately I didn’t. I hope from the tutors to explain what are our tasks, just for the first two weeks so that we get familiar with the flow and the tasks. \\nThank you again for you wonderful efforts and I hope you take my notes as a honest feedback in order to make it better for all of us :heart:', 'ts': '1661371634.064169'}, {'text': \"Please, when we talk about normalization, which transformer did you use? I used Normalizer but I've go error during the transformation (NaN or infinte ....). I used StandardScaler, but I've got error after the transformation in my model. MinMaxScaler is my last choice. But I would like you tell me which one you used. Thanks\", 'ts': '1661376004.211299'}, {'text': '<@U03T89ACUUW> <@U03UP7V9Q57>', 'ts': '1661400307.707089'}, {'text': 'you can encode the categorical data and boxplot', 'ts': '1661406265.150159'}, {'text': 'Check this site. It mighy help\\n<https://www.tutorialspoint.com/python_data_science/python_data_aggregation.htm|https://www.tutorialspoint.com/python_data_science/python_data_aggregation.htm>', 'ts': '1661238355.026149'}, {'text': '<@U03UKL27B0R> and <@U03UKGSDGSG> Thanks a lot', 'ts': '1661238390.330319'}, {'text': 'Important site for working with missing data in pandas. <https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/|https://www.geeksforgeeks.org/working-with-missing-data-in-pandas/>', 'ts': '1661238426.371919'}, {'text': \"I'm thinking that the task1.2 will use the data made in task1.1. Right?\", 'ts': '1661238935.755629'}, {'text': \"For the total data in bytes for each application, do you think that it's good to some the DL and Ul for each application?\", 'ts': '1661239760.667549'}, {'text': 'What does this mean please \"Milliseconds offset of start time for the xDR\"', 'ts': '1661240386.321919'}, {'text': 'I think is about the shifting from the start time', 'ts': '1661240800.957299'}, {'text': 'I think that is the millisecond after the end time. The total time is Endtime +  someoffset - start Time', 'ts': '1661241010.783489'}, {'text': 'I believe, you can do that.\\nYou might just need to upload the dataset to your google drive!', 'ts': '1661242486.427089'}, {'text': '<@U03UH397319>  yeah you are right, it is Bearer Id. You aggregate it (bearer id) per user', 'ts': '1661242701.190649'}, {'text': '<@U03UH397319> you can use the sum function on the series you grouped by the user', 'ts': '1661242755.385369'}, {'text': 'Hey, <@U03UJN29Y4C> like Birhanu said using the <https://drive.google.com/drive/folders/1DNvI4z6mD07FlITP2jslUoGfU5_EvjE6|CSV file> should be faster. If that is still taking time look into moving your workflow into <https://colab.research.google.com/|Google Colab>..', 'ts': '1661242788.667809'}, {'text': '<@U03UVHCV6KB> Thanks', 'ts': '1661242795.745079'}, {'text': 'Which feature did you use to make the aggregations per user? Task 1.1', 'ts': '1661242863.866769'}, {'text': 'just to be sure', 'ts': '1661242869.492389'}, {'text': \"Hey <@U03UG32J3PC>, it looks like you're gonna go on to have similar issues when your trial period ends. Travis is of course alright but maybe start looking into <https://github.com/features/actions|Github Actions>. Some of the tasks in the coming weeks will require the use of it and it offers a very generous build runtime for your workflows.\", 'ts': '1661243096.623649'}, {'text': 'i used &lt;MSISDN/Number&gt;. because I think it represents each user uniquely.', 'ts': '1661243270.941289'}, {'text': 'there may be multiple &lt;beare id &gt;per &lt;MSISDN/Number&gt;', 'ts': '1661243316.740959'}, {'text': 'I also thought that is the user feature', 'ts': '1661243390.089499'}, {'text': 'but Im not sure', 'ts': '1661243397.822589'}, {'text': 'Thank you very much <@U03V8LHPDME>', 'ts': '1661243593.301459'}, {'text': 'I used that one too', 'ts': '1661244649.929159'}, {'text': 'Please, do we have to replace the ouliers?', 'ts': '1661244669.710549'}, {'text': 'I think that if we replace them, we will change the data since they are not missing values.. What do you think?', 'ts': '1661244758.323179'}, {'text': 'why replace when u can either normalize it or scale it?\\n\\nlike simple min max scaler or log transformation will do', 'ts': '1661244832.640929'}, {'text': '<@U03U1GHT39V> Thanks dear', 'ts': '1661244870.741969'}, {'text': \"We shouldn't. You can note them in the report, but removing them will alt your findings\", 'ts': '1661244896.929409'}, {'text': 'You might use log transformation also', 'ts': '1661244924.123389'}, {'text': 'Yes.. <@U03UJN29Y4C> Thanks', 'ts': '1661244947.350309'}, {'text': '<@U03UG1Z21JP> Thanks', 'ts': '1661244973.626329'}, {'text': 'Hey <@U03UVHCV6KB>, aggregating the data is any process where you summarize it. So for the identification of users, you can use the IMEI as Yasabneh specified which is given to every mobile phone by service providers to uniquely identify devices or you can also use the MSISDN/Number which is used to identify the unique subscription on the ISP or IMSI as well. You can use multiple approaches to reach your conclusion of whether or not to buy or avoid the company (which is the core business need at hand). The four bullet points specified are correct with the field descriptions you specified.', 'ts': '1661245451.503049'}, {'text': 'any of the three unique identifiers on the data can be used. It will depend on whether or not you want to base your analysis on the apparatus -- IMEI in that case or on the subscription to the ISP --IMSI(15 number identifier). You can also use  MSISDN/Number, which is the phone number you call.', 'ts': '1661246078.251189'}, {'text': 'All 3 are used to identify users in some unique way', 'ts': '1661246103.821719'}, {'text': 'Thank you <@U03V8LHPDME>', 'ts': '1661246591.763989'}, {'text': 'Noted with thanks <@U03V8LHPDME>', 'ts': '1661246685.924609'}, {'text': \"If you take one row you'll see that the Start time of the XDR is in year-month-date hour:minute:second format. The milliseconds provide additional information for start and end of the XDR session.\", 'ts': '1661247828.470859'}, {'text': \"Thank you guys, you've been really helpful\", 'ts': '1661247940.731359'}, {'text': 'which methods you used to fill missing data??', 'ts': '1661247976.852449'}, {'text': \"Hello everyone, how are you doing?\\nI got a problem on following git workflow, I can't find fork icon in my git repo. I see the fork button from others git.\\nhow can I gate it in my own account? Is there any one who can help me please?\", 'ts': '1661248133.231699'}, {'text': 'First, I calculated the skewness and found out that most of the columns are skewed. I used mode for the skewed column.', 'ts': '1661248508.270539'}, {'text': 'Anyone from trainees  to assist on this? If none please reach out to <@U03UP7V9Q57>', 'ts': '1661248554.831579'}, {'text': \"```df_clean['race'] = df_clean['race'].fillna(df_clean['race'].mode()[0]) ```\", 'ts': '1661248581.985429'}, {'text': 'like this function ??', 'ts': '1661248595.705369'}, {'text': \"<@U03UGB3T3MY> I think you can't fork your own repo being in the same repo if it is not hosted somewhere. Forking is all about cloning hosted open source repo from within an other repo. The fork button is inactive as of mine if I am within the repo I want to fork.\", 'ts': '1661248959.589179'}, {'text': 'if you need to apply GitHub workflow this is helpful <https://docs.github.com/en/actions/using-workflows/about-workflows>', 'ts': '1661249035.940949'}, {'text': '<!here> <!channel> I appreciate if anyone can help normalizing and scaling non-numeric data in a simplest way', 'ts': '1661249098.665009'}, {'text': 'exactly', 'ts': '1661249141.423459'}, {'text': 'I do use IMEI number, even if the same person can have two or more mobiles. It worked for me', 'ts': '1661249169.279439'}, {'text': 'I think scaling and normalization are only applicable to numeric data', 'ts': '1661249170.267419'}, {'text': 'not replacing but normalizing the outliers is best possible solution', 'ts': '1661249227.995499'}, {'text': 'the transformation method depends on the type of data to be transformed', 'ts': '1661249269.097029'}, {'text': '<@U03UH397319> there are ways to normalize non-numeric data with python code but I want not to go that details. We can label the non-numeric unique data and assign numeric index so that we can normalize with a code', 'ts': '1661249357.900039'}, {'text': \"<@U03UKL27B0R> what can we do with our data normalization and scaling with 'object' types?\", 'ts': '1661249457.462909'}, {'text': 'How are we handling \"Bearer ID\" and \"MSISDN\" missing values?\\nI have been thinking of dropping  them since they are unique ids and most them have Nan values for customer id as well.', 'ts': '1661249468.363029'}, {'text': 'I think there might be a good reason to remove them. For example, the outliers could be some noise data, which will alter the findings if we do not remove it.', 'ts': '1661249537.478749'}, {'text': 'it is a bit confusing because Bearer ID is used as session id. If we drop, it means it will be difficult to drive session related insights. I am confused too but I used mode method to fill with it.:face_with_head_bandage:', 'ts': '1661249638.326819'}, {'text': 'In my case, there is a missing value in the &lt;MSISDN&gt; but not in the Bearer ID. I chose not to drop it because I used the MSISDN as a customer_id', 'ts': '1661249724.020989'}, {'text': 'mode here might mislead you I suppose.', 'ts': '1661249813.120379'}, {'text': '<@U03U9FWPNCE> How it misleads me? What to do better?', 'ts': '1661249863.671039'}, {'text': 'This make sense to me, because there is not lead to interpolating MSISDN nan values. however, they will be some bearer ids still missing.', 'ts': '1661249884.587859'}, {'text': '<https://careerfoundry.com/en/blog/data-analytics/how-to-find-outliers/#:~:text=Using%20pandas%20describe()%20to,not%20the%20dataset%20has%20outliers>.', 'ts': '1661249982.892509'}, {'text': 'I want to show the &lt;graphical univariate analysis&gt; for all the attributes in a single figure. Anyone who could tell me how to do that?', 'ts': '1661250041.001779'}, {'text': '<@U03UJGRN5E0> we might loose an information while dropping what we think of outlier - data points that deviate from the mean can be treated by normalizing or scaling', 'ts': '1661250200.346309'}, {'text': '<https://www.statology.org/matplotlib-multiple-plots/#:~:text=You%20can%20use%20the%20following%20syntax%20to%20create,show%20how%20to%20use%20this%20function%20in%20practice|https://www.statology.org/matplotlib-multiple-plots/#:~:text=You%20can%20use%20the%20following%20sy[…]w%20to%20use%20this%20function%20in%20practice>', 'ts': '1661250335.912209'}, {'text': 'the dataset has over 134000 unique values, which means that almost all the values are unique. so the most occurring value might not represent about 1000 missing values.', 'ts': '1661250644.209679'}, {'text': 'We want to normalize/standardize our data when we are ready to feed it to our model. And you would normally normalize/standardize numeric data. When you are working with object types if they are categorical data you would perform some type of encoding which will convert it to numerical form, and there will be no need to normalize that as it is in categorical form.', 'ts': '1661250772.325729'}, {'text': 'The first step is to perform an EDA before starting to process our data, and that will give as an insight of the data, detect if there are outliers, and so on.', 'ts': '1661250787.899349'}, {'text': '<@U03UKL27B0R> what about filling the missing values?', 'ts': '1661250844.591079'}, {'text': 'You can use interpolation, mean, median, or mode based on the feature and data.', 'ts': '1661250890.921409'}, {'text': 'Alright. Thanks', 'ts': '1661250936.627219'}, {'text': \"anyone who have an idea on how to use  utility function from the folder scripts, just i follow the format, from scripts.utility import func_name. it didn't work 4 me!\", 'ts': '1661251530.226549'}, {'text': 'Do you have `__init__.py` in your scripts folder?', 'ts': '1661251600.330539'}, {'text': 'Or you can try this', 'ts': '1661251647.543629'}, {'text': \"`import sys, os`\\n`sys.path.append(os.path.abspath(os.path.join('../scripts/')))`\", 'ts': '1661251655.319319'}, {'text': 'And then\\n`from utility import *`', 'ts': '1661251693.007499'}, {'text': \"I also faced this issue <@U03UUR571A5>'s answer should do\", 'ts': '1661251829.180559'}, {'text': 'But is it necessary to add `__init__.py` file? <@U03UUR571A5>', 'ts': '1661252003.177099'}, {'text': \"That was only required on older python version before 3.4 or 3.3. If you have newer version that won't be required.\", 'ts': '1661252225.097049'}, {'text': \"It is not necessary if you use `sys.path.append(os.path.abspath(os.path.join('../scripts/')))`.\\nBut if you do have `__init__.py` github actions will complain and throw an error while the tests are running for your scripts especially if you don't use the correct statements\\nfor example the statement `import utils` will be an error in github actions even though it works fine locally,\\nso you have to use `from . import utils`\", 'ts': '1661252273.376439'}, {'text': 'Hello guys, I think this link can help us: <https://www.canva.com/>', 'ts': '1661252346.115189'}, {'text': 'Thank you <@U03UKL27B0R> <@U03UUR571A5>', 'ts': '1661252520.790979'}, {'text': 'is this problem solved? if not I can really help you', 'ts': '1661252662.070009'}, {'text': 'Thanks for sharing', 'ts': '1661254076.031139'}, {'text': '<https://machinelearningmastery.com/understand-machine-learning-data-descriptive-statistics-python/>', 'ts': '1661255103.539409'}, {'text': 'I would like to kindly recommend that it would really be nice to post this kind of resource in the all-resource channel, for everyone to get the best out of each and every channel', 'ts': '1661256281.975969'}, {'text': 'Please, do we have to clean fill missing values in Bearer Id ? Since they are supposed to be unique', 'ts': '1661258827.607109'}, {'text': 'You can drop rows without Bearer Ids', 'ts': '1661258868.015369'}, {'text': 'Alright', 'ts': '1661258895.309849'}, {'text': 'so guys I am struggling to fil missing values in *Bearer Id* column.\\nhow did you solve this problem ??', 'ts': '1661259107.987329'}, {'text': 'You can drop the rows with no bearer ids', 'ts': '1661259196.806419'}, {'text': 'great', 'ts': '1661259292.215429'}, {'text': 'I thought we have a tutorial now', 'ts': '1661259752.375989'}, {'text': 'yes', 'ts': '1661259781.180419'}, {'text': 'the tutorial has started', 'ts': '1661259790.907309'}, {'text': 'Alright', 'ts': '1661259799.739089'}, {'text': 'Thanks', 'ts': '1661259820.077889'}, {'text': 'tutorial starting!', 'ts': '1661259910.835549'}, {'text': 'can you share the link', 'ts': '1661260018.836269'}, {'text': '<https://meet.google.com/ysb-kjdn-hpp>', 'ts': '1661260096.869159'}, {'text': 'It returned like this. What I want to do is add the numbers of the second column with the same value of fist column', 'ts': '1661263248.535069'}, {'text': 'Here is the snippet', 'ts': '1661263297.552489'}, {'text': 'guys could u tell me which task will be submitted for tomorrow i need detail info?', 'ts': '1661263650.026639'}, {'text': \"i think that is because you are using two columns to make 'groupby'\\ntry it by removing the col 'Gaming DL (Bytes)' from groupby parameter\", 'ts': '1661263949.246819'}, {'text': \"It is a 15 page interim report on the week 1 challenge like we did in week 0's\", 'ts': '1661265233.774359'}, {'text': '<!here> <!channel> <@U03V8LHPDME> <@U03UKL27B0R> I would appreciate if anyone help me know which variable or combination of variables can identify session frequency', 'ts': '1661266364.382279'}, {'text': \"When I've replaced the values greater than quantile 0.95, I still have some outliers on the boxplot, but they are relatively more close. How to treat them?\", 'ts': '1661266631.196079'}, {'text': '<@U03UH397319> tune for quantile ratio; may be reducing the ration may remove the remainin', 'ts': '1661266724.703549'}, {'text': 'I think that values are considered as outlier when they are greater than Q3+1.5*IQ while Q3 is the third quantile and IQ = Q3 - Q1. May be I will tune the ratio to than value. What to you think ?', 'ts': '1661267110.184759'}, {'text': 'what about github', 'ts': '1661267830.952679'}, {'text': \"I think after replacement, if I plot again, it's normal to still have outliers, because the data have been changed. And the boxplot is made for the new data. Not that there is still outliers.\", 'ts': '1661268780.865039'}, {'text': 'have you tried to replace values greater than quantile 0.90 (or any value less than 0.95)?', 'ts': '1661269091.307699'}, {'text': \"<@U03UJKJGRAQ> I've tried 0.80. But some are having more outliers than before and some are not having anyone\", 'ts': '1661269440.097889'}, {'text': 'Implementation of task 1 is expected from your Github submission.', 'ts': '1661269667.680569'}, {'text': 'You will be using the total session duration based on data description that was provided to get the duration.', 'ts': '1661269721.680499'}, {'text': \"<@U03UH397319> I have not tried myself. But I think you're correct, replacing outliers changes the distribution. So it makes sense that more outliers pop out each time.\", 'ts': '1661269733.306479'}, {'text': \"If the outliers aren't affecting the total distribution of your data you can skip it, but if it still affects the distribution you need to handle the remaining outliers.\", 'ts': '1661269898.035689'}, {'text': '<@U03UKL27B0R> So I should have a distribution with no outliers before continuing', 'ts': '1661270056.022069'}, {'text': \"If the outliers aren't on the other extreme or affecting your analysis that would be okay. But if it's still out of the normal range or extreme you need to handle them.\", 'ts': '1661270149.367969'}, {'text': 'Thank you <@U03UKL27B0R>. I will try it accordingly.', 'ts': '1661270573.793599'}, {'text': \"<@U03V1AM5TFA> <@U03TEPYRM2P> Isn't there any submission today? I was expecting task 2 as today's submission. Is that not?\", 'ts': '1661270900.788979'}, {'text': 'There are two submissions for this week. Interim and final submission. Tomorrow is the deadline for the interim submission.', 'ts': '1661270975.765039'}, {'text': 'Thank you <@U03TEPYRM2P> and <@U03UVHCV6KB>. I got it.', 'ts': '1661271039.390009'}, {'text': 'Kindly check the deliverable sub-topic in the challenge document - you will see what you need to submit', 'ts': '1661271040.230109'}, {'text': 'You can also refer on the week 1 schedule to have a clear sense of the submissions deadlines for both technical or non-technical. For technical, we have two submissions (interim submission and Final submission - kindly check the schedule and challenge doc)', 'ts': '1661271655.425389'}, {'text': \"I appreciate your help. I was taking daily submissions as a trend from week 0. As a result, I tried to hurry up on yesterday's tasks posted on classroom. That was misconception but it helped me a lot today. I covered basic concepts yesterday. Thank you.\", 'ts': '1661271891.215409'}, {'text': \"that's good!\", 'ts': '1661272085.779849'}, {'text': 'How do you handle missing values for handset manufacturer and handset type', 'ts': '1661272720.780049'}, {'text': 'since it is non numeric data, I used mode method', 'ts': '1661272758.567839'}, {'text': \"Can't I just fill it with something else?\", 'ts': '1661273155.616489'}, {'text': 'you can but you should be sure that the method fits well for your data type.', 'ts': '1661273205.570349'}, {'text': \"Even after dropping some columns, it doesn't reflect in my output, what am I doing wrong please\", 'ts': '1661273270.643799'}, {'text': 'Oh, yeah, that true. Thank you', 'ts': '1661273304.872149'}, {'text': '<!here> Anyone who understood what \"Activity Duration\" feature is exactly. What is its meaning?', 'ts': '1661273341.861859'}, {'text': 'have you considered the axis? is that what you want?', 'ts': '1661273470.210069'}, {'text': 'could I have todays tutorial video ???', 'ts': '1661273565.776729'}, {'text': 'kindly check the <#C03U4J8J4LQ|all-broadcast> channel', 'ts': '1661273599.504269'}, {'text': 'The axis is there.', 'ts': '1661273653.042739'}, {'text': 'thank you I found it', 'ts': '1661273666.440049'}, {'text': 'may be try inplace=True', 'ts': '1661273666.957459'}, {'text': 'Or assign it to df = df.drop()', 'ts': '1661273688.900029'}, {'text': 'Try `df = df.drop(cols_to_remove, axis=1)`', 'ts': '1661274104.235129'}, {'text': 'I think it is xDR duration excluding inactive periods', 'ts': '1661274319.437429'}, {'text': 'Thank you guys, df= .... Works', 'ts': '1661274798.874419'}, {'text': 'How do you drop rows with NaN values with out dropping corresponding columns?', 'ts': '1661275475.576199'}, {'text': 'set the axis to 0 - row wise', 'ts': '1661275517.135009'}, {'text': \"you may change the axis to 0 or 'index'\", 'ts': '1661275518.044549'}, {'text': 'Okay but i wanted to drop them along a particular feature', 'ts': '1661275890.109329'}, {'text': 'if i did standardization all the outliers will vanish, so what the purpose of plotting them ??', 'ts': '1661277161.787219'}, {'text': \"One moment my notebook will work, the next moment it's not working. If you're using anaconda jupyter and you know the way out of this, please let me know\", 'ts': '1661277300.214789'}, {'text': 'Is it import issue you are facing?', 'ts': '1661277504.146669'}, {'text': 'Not really', 'ts': '1661277542.635099'}, {'text': 'It looks like a cache issue', 'ts': '1661277553.158989'}, {'text': \"I have rerun every cell, so it's working again now\", 'ts': '1661277587.048799'}, {'text': '<@U03UJGP0C68> will we do the same for each application ?', 'ts': '1661279212.345329'}, {'text': 'thanks a lot Yididya', 'ts': '1661280808.105759'}, {'text': 'I am wondering if  for task 1.2 we will be working on the columns of task 1.1 or on the whole dataset.', 'ts': '1661281249.455029'}, {'text': 'guys I am struggling in Standardization and normalization\\nwhat I should do?', 'ts': '1661282045.957469'}, {'text': 'I think on the columns of task 1.1', 'ts': '1661284153.711849'}, {'text': 'What do you need?', 'ts': '1661284190.781539'}, {'text': 'thsnk you <@U03UH397319>', 'ts': '1661284896.158899'}, {'text': 'sometimes its the environment you are working in. If you have multiple environments you must be sure wch environment has the modules you are using and so on.', 'ts': '1661285954.554959'}, {'text': 'and sometimes as you said it is a cache issue. you have got to restart the whole notebook if you change something outside of that notebook that is referenced or imported by that notebook. If you have any reference or import and you made changes to the references your notebook will not reflect the changes unless you restart it.', 'ts': '1661286062.351249'}, {'text': 'I think you will do it the whole data set', 'ts': '1661286094.346649'}, {'text': 'the difference is not too much, better to do it on the whole dataset', 'ts': '1661286116.792559'}, {'text': \"plus as the team said on the first day, you need to have a strategy, if you are stuck on a problem and it takes too long to solve, you actually need to have some strategy in place to maneuver the error, otherwise, you might not be able to do other tasks well do to two things. 1 is burnout, you might feel really uncomfortable continuing to do the tasks. And the 2nd is time, you might have done several easier and more manageable tasks in the time you put in fixing the error. So as they said it's better to put some kind of strategy.\", 'ts': '1661286348.796329'}, {'text': 'I chose the columns of task 1 because of the objectives of the task: To have an idea of the users behavior on the different applications. The other columns in the whole dataset are not related to that', 'ts': '1661286687.482459'}, {'text': '<@U03UG1Z21JP> sure. I used this technique', 'ts': '1661287444.343449'}, {'text': 'ok', 'ts': '1661287619.301439'}, {'text': 'Think you are talking about pipelines', 'ts': '1661288740.588199'}, {'text': \"But in your case, you just want to do standardization and some small things, I don't know. You can just create some scripts, like they said, that you can be using. That is the usefulness of scripts\", 'ts': '1661288826.341259'}, {'text': '<https://www.youtube.com/watch?v=J_6ZwHCT558>', 'ts': '1661290517.867469'}, {'text': \"<@U03UVHCV6KB> Well, I've only be working on one environment since i download anaconda. It's frustrating having to rerun all the cells all the time.\", 'ts': '1661316982.775449'}, {'text': 'How can we get top 10 of the column if the dtype is object? Do I have to change the data type?', 'ts': '1661318399.522209'}, {'text': 'What are you guys using for your work please, jupyter from anaconda is giving some troubles?', 'ts': '1661318940.506229'}, {'text': 'check for the most frequent ones, probably by using groupby', 'ts': '1661319013.612679'}, {'text': 'Install jupyter separately.', 'ts': '1661319527.174769'}, {'text': 'What is the problem with that', 'ts': '1661319831.607049'}, {'text': 'One moment it will work, the next moment it wont.', 'ts': '1661320427.410919'}, {'text': 'Did you use the same environment in both times?', 'ts': '1661320480.020249'}, {'text': 'Yes I do, these things happens in seconds, run a cell, it give a desirable result, run it again, you get an error.', 'ts': '1661320681.838859'}, {'text': '<@U03UFV7TUTV>', 'ts': '1661320704.952919'}, {'text': 'is that what you use? do you have a good experience with that?', 'ts': '1661320746.181199'}, {'text': 'Can you show us the screenshot of the error', 'ts': '1661320855.417569'}, {'text': \"When I use groupby I think it's just sorting them alphabetically. How exactly can I find the freqency?\", 'ts': '1661320900.904159'}, {'text': '<@U03UJN29Y4C>   yeah pip install jupyter and install other requirement modules', 'ts': '1661320974.924649'}, {'text': '<@U03UJN29Y4C>', 'ts': '1661321034.356249'}, {'text': 'use the value_counts() <@U03UAKATQ22>', 'ts': '1661322591.449609'}, {'text': '<https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html>', 'ts': '1661322614.721329'}, {'text': \"I think you need one on one session from to help you either install it via terminal or use vs code's jupyter notebook\", 'ts': '1661323235.614189'}, {'text': \"Thank you so much guys, I'll do the manual installation later. for now I've switched to google colab just to finish up my work\", 'ts': '1661323518.813419'}, {'text': \"Thank very much <@U03UG32J3PC>, I'm still gonna dm though :see_no_evil:\", 'ts': '1661583647.138179'}, {'text': 'Thank you.', 'ts': '1661583676.608569'}, {'text': 'shall we submit same link for github dashboard link and  data analysis coding? <@U03TX2VN6H5> or should we use a separate repository for the dashboard?', 'ts': '1661588504.974709'}, {'text': 'What do instructions say <@U03UJGP0C68> ? <@U03TEPYRM2P> <@U03UP7V9Q57>', 'ts': '1661589284.025839'}, {'text': \"it doesn't explicitly say\", 'ts': '1661589473.272939'}, {'text': 'I need some help here.. I can see what is wrong', 'ts': '1661590445.818359'}, {'text': 'running \"streamlit hello\" on cmd gives nice hands on demos', 'ts': '1661592396.746889'}, {'text': 'Please help me..', 'ts': '1661593369.659219'}, {'text': \"as i understand from the error you don't have a column named msisdn_number\", 'ts': '1661593720.328079'}, {'text': 'But', 'ts': '1661594054.001449'}, {'text': \"Could you check it for me ? I don't understand because it's supposed to be there\", 'ts': '1661594073.941029'}, {'text': 'you executed this method? or did you pass df to be added to the table?', 'ts': '1661594639.377169'}, {'text': \"can you confirm you passed 'userinformation' as a table name?\", 'ts': '1661594946.569549'}, {'text': 'You will submit a single Repo. You can use a fit submodule for the dashboard if you want. ', 'ts': '1661596803.948949'}, {'text': 'nice. I did not know that, thanks', 'ts': '1661597347.156499'}, {'text': 'thank you', 'ts': '1661597451.688879'}, {'text': 'how I suppose to calculate Average\\n• TCP retransmission\\n• Average RTT\\n• Average throughput', 'ts': '1661600319.274819'}, {'text': '<@U03UKL27B0R>', 'ts': '1661600367.554779'}, {'text': 'by summing columns (\\n```TCP DL Retrans. Vol (Bytes)      \\nTCP UL Retrans. Vol (Bytes) ```\\n', 'ts': '1661604605.863089'}, {'text': \"```'Avg RTT DL (ms)'\\n'Avg RTT UL (ms)'```\\n\", 'ts': '1661604652.200029'}, {'text': \"```'Avg Bearer TP DL (kbps)'\\n'Avg Bearer TP UL (kbps)```\\n\", 'ts': '1661604684.045989'}, {'text': 'group them using the user and use the agg() function of a data frame and pass the way you want to use the agg() function, like mean, sum . . .\\n\\n<https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html>', 'ts': '1661606306.316299'}, {'text': \"You know you are bad at something and need to improve when it takes you a day to complete it while others do it in two hours or three. [I'm talking about myself by the way]\", 'ts': '1661606392.403429'}, {'text': 'You are not alone:people_hugging:', 'ts': '1661606439.374989'}, {'text': 'I got it thank you ', 'ts': '1661606929.114089'}, {'text': 'It just feels you are talking about me! :cry:', 'ts': '1661607003.398929'}, {'text': 'Same here ', 'ts': '1661607003.984439'}, {'text': 'Has anyone managed to run their multiapp dashboard to help me?', 'ts': '1661609364.859759'}, {'text': \"Hello, can someone explain how we're supposed to use Euclidean distance to get the satisfaction score?\", 'ts': '1661609589.698739'}, {'text': 'Hey, I could not reach my paired mentor now what should I do ?', 'ts': '1661613947.657559'}, {'text': 'Reach out to any other trainee from the list at random.', 'ts': '1661614102.521709'}, {'text': \"I think it's the difference in magnitude between each data and the centroid of the cluster that data point lies during the kmean\", 'ts': '1661614891.359739'}, {'text': 'You do this for both experience and engagement and average them to get the satisfaction. ', 'ts': '1661614957.199399'}, {'text': 'Thank you.', 'ts': '1661614980.768799'}, {'text': \"Good if it makes sense. But I'm not so sure\", 'ts': '1661615017.773289'}, {'text': 'what do you mean by multi app?', 'ts': '1661615062.579209'}, {'text': 'okay', 'ts': '1661615441.398379'}, {'text': 'Which features are we supposed to use to train our regression model for task 4?', 'ts': '1661617622.267539'}, {'text': 'hey <@U03UL5LSTG9>, I just finished a call with <@U03UJKJGRAQ>. as I should write my report, does Emtinan have to write or update her report ??', 'ts': '1661617642.150409'}, {'text': \"You can go ahead with your report; she'll write her report as she sees fit. \", 'ts': '1661617798.088489'}, {'text': '<@U03UL5LSTG9> Good afternoon. I already had a report for the Peer Mentor Exercise with my original mentor. But I guess that I can update my report to include my meeting with <@U03V6HMRPGQ> as well.', 'ts': '1661618361.970159'}, {'text': 'Sure, you can include both. ', 'ts': '1661618807.965419'}, {'text': 'can anyone help me on this error?', 'ts': '1661619485.169149'}, {'text': 'yes, exactly', 'ts': '1661621292.657089'}, {'text': 'could some one help me by explaining task 4 ??', 'ts': '1661625062.645389'}, {'text': '<@U03UKL27B0R>', 'ts': '1661625121.910339'}, {'text': 'I think you are trying to acess a value that is out of the list. I thing the index you use &gt; the length of the list', 'ts': '1661625169.261759'}, {'text': '<@U03TEPYRM2P>', 'ts': '1661625170.958739'}, {'text': 'Thanks <@U03UG32J3PC>', 'ts': '1661625721.495269'}, {'text': 'Task 4 requires you to first calculate cluster engagement and experience for users and calculating their scores. Then based on scores of engagement and experience you can get satisfaction score which is the average of engagement and experience score. Finally you will be building a regression model to predict satisfaction score of customer based on the engagement and experience score given.', 'ts': '1661626530.599469'}, {'text': 'how I am going to calculate the cluster engagement and experience for users and calculating their scores?', 'ts': '1661626849.352079'}, {'text': 'Clustering users is an unsupervised learning and then as specified in the challenge doc, you will get the score  using the euclidean distance. Satisfaction  score is the average of experience score and engagement score.', 'ts': '1661627650.327119'}, {'text': 'Are we supposed to include screenshots of our dashboard to the final report? <@U03TEPYRM2P>', 'ts': '1661628615.670859'}, {'text': '<@U03UKL27B0R> <@U03U9DB7REG> ', 'ts': '1661628982.627849'}, {'text': 'Your final report includes limited number of slides. Make sure to only add relevant findings of your data as it would be a report or presentation to your employer or manager.', 'ts': '1661629467.510289'}, {'text': '<https://www.section.io/engineering-education/how-to-deploy-streamlit-app-with-docker/>', 'ts': '1661638084.234439'}, {'text': 'okay. thank you', 'ts': '1661638632.171329'}, {'text': \"Can I get more info on the where the telecommunications dataset comes from? I'd like to put the project on my resume.\", 'ts': '1667932405.677889'}, {'text': '<@U03V8LHPDME> <@U03UKL27B0R> <@U03U9DB7REG> a little clarification on what link we are submitting for Interim please', 'ts': '1661979057.581639'}, {'text': 'PDF file format', 'ts': '1661979653.154169'}, {'text': \"hope I'm not too late :crossed_fingers:\", 'ts': '1661979680.826439'}, {'text': 'You will be using the report submission link to submit the report and github submission link to submit your github repo.', 'ts': '1661979683.761039'}, {'text': 'I meant for the github link ', 'ts': '1661979690.936839'}, {'text': 'The org. Or my repo', 'ts': '1661979754.550649'}, {'text': 'the github repo you and your group collaborated on, Am I missing something here ? Thought that was straight forward', 'ts': '1661979765.042359'}, {'text': 'oh ok', 'ts': '1661979772.260639'}, {'text': 'the org', 'ts': '1661979775.690969'}, {'text': \"You can fork the organizational repo and submit the forked one or the organizational repo itself. It won't matter as long as we can see your work,\", 'ts': '1661979865.548709'}, {'text': 'Okay thanks', 'ts': '1661979934.403769'}, {'text': '<@U03UDBUL7CL> has joined the channel', 'ts': '1660893924.224059'}, {'text': '<@U03U4GULU3Y> has joined the channel', 'ts': '1660897470.000909'}, {'text': '<@U03V1AM5TFA> has joined the channel', 'ts': '1660904827.029069'}, {'text': '<@U03TX2VN6H5> has joined the channel', 'ts': '1660904837.473839'}, {'text': '<@U03U93GNNVB> has joined the channel', 'ts': '1660911924.663659'}, {'text': '<https://towardsdatascience.com/how-to-fill-missing-data-with-pandas-8cb875362a0d|https://towardsdatascience.com/how-to-fill-missing-data-with-pandas-8cb875362a0d>', 'ts': '1661158502.784099'}, {'text': 'Imputation Using k-NN, Imputation Using Multivariate Imputation by Chained Equation (MICE), and Imputation Using Deep Learning is also good for filling missing values, but they work for numeric data.', 'ts': '1661158692.172369'}, {'text': '<https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779>', 'ts': '1661158702.777369'}, {'text': 'u can use SMOTE or mode to impute categorical dataset', 'ts': '1661158767.626859'}, {'text': ':+1:thanks', 'ts': '1661159006.632649'}, {'text': '<https://www.scribbr.com/statistics/normal-distribution/#:~:text=In%20a%20normal%20distribution%2C%20data%20are%20symmetrically%20distributed%20with%20no,same%20in%20a%20normal%20distribution>.', 'ts': '1661160070.855719'}, {'text': 'Any have information, on the time the next session will be started?', 'ts': '1661161636.759979'}, {'text': 'Please check the schedule', 'ts': '1661161669.054589'}, {'text': '<https://www.notion.so/10academy/10-Academy-Batch-6-week-1-Schedule-1c117d808d5846a9858168cb23aa3cee|https://www.notion.so/10academy/10-Academy-Batch-6-week-1-Schedule-1c117d808d5846a9858168cb23aa3cee>', 'ts': '1661161725.970779'}, {'text': 'Thanks Natnael', 'ts': '1661161916.102399'}, {'text': 'Thank you Amanuel', 'ts': '1661162294.751419'}, {'text': '<@U03UG5VFN03> this may help you...\\nAlso if you are from Ethiopia...use the following simple math to go on with the schedule....', 'ts': '1661162413.996529'}, {'text': '<!channel> please note that our core hours range from 8:00 AM - 2:00 PM UTC. This is a period where you can ask any questions and expect a quick response from the team or colleagues. I would like to encourage everyone to be more active and contributing on slack during these core hours. Thanks and keep enjoying the challenge!!!', 'ts': '1661165926.664459'}, {'text': 'Noted with thanks', 'ts': '1661166153.408319'}, {'text': \"in other words - don't struggle alone for a long time (You have people)!\", 'ts': '1661166527.221209'}, {'text': '<https://tenx.10academy.org/trainee> , I bookmarked this link, but it is not working these couple of days\\nIt is constantly giving me this default error page.\\nWhat seems to be the problem here?\\n<@U03UYNR4TS4>', 'ts': '1661168148.023629'}, {'text': '<@U03UVHCV6KB> Same here... I think the account we were given works just for week 0.', 'ts': '1661168279.595709'}, {'text': '<https://www.youtube.com/watch?v=ff5cBkPg-bQ>', 'ts': '1661168294.408199'}, {'text': '<@U03UYNR4TS4> will help!', 'ts': '1661168365.379509'}, {'text': 'are we suppose to get a new tenx account?', 'ts': '1661168445.894949'}, {'text': 'Try it in incognito window, works fine for me in incognito.', 'ts': '1661168446.266669'}, {'text': 'Thanks <@U03UJGP0C68> worked for me incognito too', 'ts': '1661168584.270899'}, {'text': 'The project structure that is suggested: <https://github.com/10xac/PythonPackageStructure>', 'ts': '1661168608.451289'}, {'text': '<@U03UVHCV6KB> try it incognito', 'ts': '1661168617.322889'}, {'text': 'I had the same issue until I cleared the cache on my browser.', 'ts': '1661168840.037709'}, {'text': 'Okay thanks I will try it', 'ts': '1661170044.469879'}, {'text': 'You can try by clearing cache', 'ts': '1661170102.554559'}, {'text': 'remove the cookies from your browser on this website(<https://tenx.10academy.org/trainee>) and reload\\nincognito is not required', 'ts': '1661170776.163109'}, {'text': 'Thanks everyone I will check it out', 'ts': '1661171818.664849'}, {'text': 'You can follow this to solve the problem\\n<https://10academy.notion.site/Guide-for-removing-cache-and-cookies-04ed51fc65084ca7b0a2a0a7eaf055dd>', 'ts': '1661172534.721249'}, {'text': 'Can we have the last session recording, please?', 'ts': '1661177060.092759'}, {'text': 'Hey guys is there a link to week 1 Google drive yet ??', 'ts': '1661178181.592079'}, {'text': '<https://drive.google.com/drive/folders/1ctMjDWwZfEc84GM5Oca4y6Zp9Q2t_Og1>', 'ts': '1661178212.322989'}, {'text': 'Thanks man', 'ts': '1661178346.839989'}, {'text': 'Hi guys, <@U03TEPYRM2P>, should we submit our GitHub repo today and start working on that or are we supposed to perform(complete) the tasks before submitting the link', 'ts': '1661178576.148629'}, {'text': 'There are only two submissions this week. On Wednesday and Saturday..\\nKindly refer to the challenge document on this', 'ts': '1661178725.155779'}, {'text': 'alright, thanks', 'ts': '1661178865.267149'}, {'text': 'It will be uploaded very soon.', 'ts': '1661185419.088949'}, {'text': 'Thanks!', 'ts': '1661188721.054979'}, {'text': 'It takes more than two minutes for my excel file to load into a dataframe, any idea how to speed this up?', 'ts': '1661192544.684499'}, {'text': 'why not you use the csv and mount on drive colab?', 'ts': '1661192600.200769'}, {'text': 'Use csv file', 'ts': '1661192613.214279'}, {'text': 'There is a csv version of the provided data?', 'ts': '1661192789.707979'}, {'text': '<https://drive.google.com/file/d/1xmUwEvcOzQbeRyPJycqSK1WkC3pos2II/view?usp=sharing>', 'ts': '1661192857.579359'}, {'text': 'where is that?', 'ts': '1661192859.790809'}, {'text': '<https://drive.google.com/drive/folders/1DNvI4z6mD07FlITP2jslUoGfU5_EvjE6>', 'ts': '1661192890.576239'}, {'text': 'Thank you', 'ts': '1661193060.823449'}, {'text': 'Thank you', 'ts': '1661193094.997079'}, {'text': 'yeah', 'ts': '1661195065.724499'}, {'text': '<https://drive.google.com/file/d/1xmUwEvcOzQbeRyPJycqSK1WkC3pos2II/view?usp=sharing>', 'ts': '1661195111.197259'}, {'text': '<@U03TEPYRM2P> When can we expect to have the video recording!', 'ts': '1661196474.741429'}, {'text': 'i think, its released on youtube.', 'ts': '1661197321.499969'}, {'text': 'any link?', 'ts': '1661197368.717229'}, {'text': '<https://www.youtube.com/watch?v=_-FqCUUWcUA>', 'ts': '1661197378.356569'}, {'text': \"All today's videos are available there\", 'ts': '1661197409.516969'}, {'text': 'Thank you!', 'ts': '1661197501.905789'}, {'text': '<https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/>', 'ts': '1661197933.808999'}, {'text': 'this is very helpful article', 'ts': '1661197965.747909'}, {'text': 'I got this error in Travis ci \"We are unable to start your build at this time. You exceeded the number of users allowed for your plan. Please review your plan details and follow the steps to resolution.\". Anyone who has experienced this error?', 'ts': '1661199187.346739'}, {'text': 'love to  work on new errors, can you post a screenshot, I will try my best', 'ts': '1661199847.221709'}, {'text': 'That was the error on my travis account, But now I have talked with the Travis guys and they assigned me a trial plan', 'ts': '1661201101.166529'}, {'text': '', 'ts': '1661201127.363639'}, {'text': 'Can someone please explain to me what is expected here:', 'ts': '1661202140.528449'}, {'text': 'ok', 'ts': '1661205398.998559'}, {'text': 'I understand this to be a task where we aggregate the 4 things listed in bullets based on each users or per users', 'ts': '1661205461.911939'}, {'text': 'My question however is which feature are we supposed to use as users?', 'ts': '1661205491.306189'}, {'text': \"for the four bullets, I think we can use\\n• number of xDR sessions = 'Bearer Id'\\n• Session duration = 'Dur. (ms)' or 'Dur. (s)'\\n• the total download (DL) and upload (UL) data = 'Total DL (Bytes)' and 'Total UL (Bytes)'\\n• the total data volume (in Bytes) during this session for each application = aggregate for all the applications provided \\n\", 'ts': '1661205744.819489'}, {'text': 'Which feature did you use to make the aggregations per user? Task 1.1', 'ts': '1661205791.692899'}, {'text': 'Yes... Now how to identify the users ?', 'ts': '1661206008.044959'}, {'text': \"I'm looking for it since? which feature is user ???\", 'ts': '1661206039.704709'}, {'text': 'A user can have unique mobile number or phone. I used IMEI to identify the used but you may use MSISDN/Number or IMSI', 'ts': '1661206478.449789'}, {'text': '<@U03UVHCV6KB> you did the right way.', 'ts': '1661206563.688359'}, {'text': \"<@U03UKL27B0R> I have used sum but it didn't sum the values. I want to add the duration with the same id in the first column. I didn't add.\", 'ts': '1661206897.792509'}, {'text': '', 'ts': '1661206941.583749'}, {'text': \"<@U03UVHCV6KB> Is the number of xDR session 'Bearer Id'?\\nI thought it will be the numbers of session per user, I mean the number of of Bearer Id for each user (a value count)\", 'ts': '1661208553.948499'}, {'text': 'First you need to group by IMEI number then apply aggregate function on the desired column \\n`agg({“col_name”:”operation”})`\\nU can use sum as operation here', 'ts': '1661227056.086099'}, {'text': 'Does the &lt;number of xDR sessions&gt; task 1.1  refers to &lt;Total Duration of the xDR&gt;?', 'ts': '1661233227.105789'}, {'text': \"Isn't that the count of the the XDR sessions?\", 'ts': '1661233501.071449'}, {'text': 'should I count bearer id per IMEI?', 'ts': '1661234172.834299'}, {'text': 'Yes <@U03UJGRN5E0>. You will count the number of sessions by the field that uniquely identifies each user.', 'ts': '1661235018.402809'}, {'text': '<@U03UKL27B0R> How can I count it ?', 'ts': '1661237182.203859'}, {'text': \"Can someone help me with the aggregation? I'm not having good idea about how to do it\", 'ts': '1661237526.480599'}, {'text': '<https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html>', 'ts': '1661237774.761319'}, {'text': 'You African telecomms data', 'ts': '1668158796.869069'}, {'text': 'All right, thanks', 'ts': '1668159386.385809'}, {'text': 'Same for me:expressionless:', 'ts': '1661676323.968759'}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m get_channel_messages(\u001b[39m\"\u001b[39m\u001b[39mall-week1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_top_topics(df)\n",
      "\u001b[1;32m/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_top_topics\u001b[39m(df):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df,word_list, word_to_id, corpus \u001b[39m=\u001b[39m prepare_data(df)\n",
      "\u001b[1;32m/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(df):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(preprocess_text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     sentence_list \u001b[39m=\u001b[39m [tweet \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mcleaned_text\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/babi/Desktop/10academy/Slack-Messages-Analysis/notebooks/topic_modeling.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     word_list \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39msplit() \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentence_list]\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "df = get_channel_messages(\"all-week1\")\n",
    "get_top_topics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenx_week0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
